{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes - Trabalho\n",
    "\n",
    "## Questão 1\n",
    "\n",
    "Implemente um classifacor Naive Bayes para o problema de predizer a qualidade de um carro. Para este fim, utilizaremos um conjunto de dados referente a qualidade de carros, disponível no [UCI](https://archive.ics.uci.edu/ml/datasets/car+evaluation). Este dataset de carros possui as seguintes features e classe:\n",
    "\n",
    "** Attributos **\n",
    "1. buying: vhigh, high, med, low\n",
    "2. maint: vhigh, high, med, low\n",
    "3. doors: 2, 3, 4, 5, more\n",
    "4. persons: 2, 4, more\n",
    "5. lug_boot: small, med, big\n",
    "6. safety: low, med, high\n",
    "\n",
    "** Classes **\n",
    "1. unacc, acc, good, vgood\n",
    "\n",
    "## Questão 2\n",
    "Crie uma versão de sua implementação usando as funções disponíveis na biblioteca SciKitLearn para o Naive Bayes ([veja aqui](http://scikit-learn.org/stable/modules/naive_bayes.html)) \n",
    "\n",
    "## Questão 3\n",
    "\n",
    "Analise a acurácia dos dois algoritmos e discuta a sua solução."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Resolução\n",
    "\n",
    "## Questão 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-daa1e40f18c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mposix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses_unique\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclasse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mclasses_unique\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclasse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[0mposix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from statistics import mean\n",
    "\n",
    "columns = [\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\", \"class\"]\n",
    "features = [\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\"]\n",
    "map_class = {0:'acc', 1:'good', 2:'unacc', 3:'vgood'}\n",
    "dataset = pd.read_csv(\"carData.csv\", names=[\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\", \"class\"])\n",
    "\n",
    "# Categorizando os dados\n",
    "for feature in columns:\n",
    "    dataset[feature] = dataset[feature].astype('category')\n",
    "    dataset[feature] = dataset[feature].cat.codes\n",
    "\n",
    "\n",
    "classes = [[], [], [], []]\n",
    "\n",
    "# separando por classe\n",
    "for row in dataset['class'].unique():\n",
    "    classes[row].append(dataset[dataset['class'] == row])\n",
    "\n",
    "#separando cada valor de cada feature pra cada classe\n",
    "classes_unique = [[], [], [], []]\n",
    "classes_feature_value = [[], [], [], []]\n",
    "probs = [[], [], [], []]\n",
    "\n",
    "for i in range(0, len(classes)):  \n",
    "    df = classes[i][0]\n",
    "    for ft in features:   \n",
    "        # Classes_unique vai ser usado para mapear os valores direito, já que não estão ordenados\n",
    "        classes_unique[i].append(df[ft].unique())\n",
    "        classes_feature_value[i].append(df[ft].unique())\n",
    "        probs[i].append(df[ft].unique())\n",
    "\n",
    "######################### Gerando a contagem de cada elemento de cada feature para cada classe #########################\n",
    "# Percorre as classes\n",
    "for i in range(0, len(classes)):  \n",
    "    df = classes[i][0]\n",
    "    # Percorre as features dentro da classe\n",
    "    for ft in range(0, len(features)):\n",
    "        # Percorrer os valores de dentro das features\n",
    "        for col in range(0, len(classes_unique[i][ft])):\n",
    "            # isso aqui eu pego o valor do elemento que a feture ft pode ter\n",
    "            elemento = classes_unique[i][ft][col]\n",
    "            # Pego o tamanho de linhas do dataset retornado por essa consulta em que o valor da linha para o atributo = elemento\n",
    "            classes_feature_value[i][ft][col] = len(df[df[features[ft]] == elemento])\n",
    "            probs[i][ft][col] = len(df[df[features[ft]] == elemento])\n",
    "\n",
    "###############################################################################################################\n",
    "        \n",
    "for i in range(0, len(classes)):  \n",
    "    df = classes[i][0]\n",
    "    for ft in range(0, len(features)):\n",
    "        probs[i][ft] = probs[i][ft].astype(float)\n",
    "        for col in range(0, len(classes_unique[i][ft])):\n",
    "            # isso aqui eu pego o valor do elemento que a feture ft pode ter\n",
    "            # Pego o tamanho de linhas do dataset retornado por essa consulta em que o valor da linha para o atributo = elemento\n",
    "            probs[i][ft][col] = probs[i][ft][col].astype(float)/sum(classes_feature_value[i][ft])\n",
    "#probs\n",
    "\n",
    "probabilidade_classes = [len(classe[0])/len(dataset) for classe in classes]\n",
    "\n",
    "##################################################### \n",
    "results = []\n",
    "X = [1, 1, 1, 1, 1, 1]\n",
    "# Ajeitar essa última parte que tá dando erro, mas a lógica tá correta\n",
    "# Documentar todo esse código\n",
    "for classe in probs:\n",
    "    produtorio = 1\n",
    "    for feature in range(0, len(features)):\n",
    "        posix = 0\n",
    "        for i in range(0, len(classes_unique[classe][0][feature])):\n",
    "            if(X[feature] == classes_unique[classe][0][feature][i]):\n",
    "                posix = i\n",
    "                break\n",
    "        produtorio *= probs[classe][feature][posix]\n",
    "    results.append(produtorio)\n",
    "\n",
    "for i in range(0, len(results)):\n",
    "    results[i] = results[i] * probabilidade_classes[i]\n",
    "max(results)\n",
    "    \n",
    "# retorna o valor maximo de results\n",
    "    \n",
    "# X = [0.5, 0.6, 0.7, 1, 2, 3] => predizer a classe\n",
    "# \n",
    "# p(c|x) = P(X|C)*P(C) / P(X) => P(X|C)*P(C)\n",
    "# MAIOR P(C|X) ai a gente retorna que é a c\n",
    "\n",
    "# probabilidade da classe = p(c) => len(classe)/numero de linhas do dataset\n",
    "# probabilidade do elemento x dado a classe = p(x|c) = p(x[0]|c)*p(x[1]|c)*p(x[2]|c)*...*p(x[5]|c)  \n",
    "# X = [0.5, 0.6, 0.7, 1, 2, 3]\n",
    "# probabilidade do elemento x = p(x)\n",
    "\n",
    "# probabilidade de cada elemento para cada feature para cada classe => elemento/len(classe)\n",
    "# os valores da classe estão ordenados de 0 a 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class NaiveBayes:\n",
    "    sum_classes = {}\n",
    "    \n",
    "    def __init__(self, filename, columns):\n",
    "        self.dataset = pd.read_csv(filename, names=columns)\n",
    "    \n",
    "    def categorizationData(self):\n",
    "        for feature in columns:\n",
    "            self.dataset[feature] = self.dataset[feature].astype('category')\n",
    "            self.dataset[feature] = self.dataset[feature].cat.codes\n",
    "        \n",
    "    def splitData(self, percent):\n",
    "        trainSize = int(len(dataset) * percent)\n",
    "        trainSet = []\n",
    "        testSet = list(self.dataset)\n",
    "        while len(trainSet) < trainSize:\n",
    "            index = random.randrange(len(copy))\n",
    "            trainSet.append(testSet.pop(index))\n",
    "        return [trainSet, testSet]\n",
    "    \n",
    "    def separetedClasses(self):\n",
    "        # Montando o sumário pelas classes\n",
    "        summary_by_class = {0:[], 1:[], 2:[], 3:[]}\n",
    "        for i in range(0, dataset.shape[0]):\n",
    "            array = [x for x in np.array(dataset.iloc[i])]\n",
    "            summary_by_class[array[-1]].append(array)\n",
    "        return summary_by_class\n",
    "    \n",
    "    def summarizeClasses(self):\n",
    "        return 0\n",
    "    \n",
    "    def prediction(self):\n",
    "        return 0\n",
    "    \n",
    "    def comparePredictions(self):\n",
    "        return 0\n",
    "    \n",
    "    def accuracy(self):\n",
    "        return 0\n",
    "    \n",
    "nb = NaiveBayes(\"carData.csv\", columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6040462427745664, 0.6069364161849711, 0.6011560693641619, 0.630057803468208, 0.6069364161849711]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.54      0.15        13\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.80      0.81      0.81       236\n",
      "           3       1.00      0.12      0.22        97\n",
      "\n",
      "   micro avg       0.61      0.61      0.61       346\n",
      "   macro avg       0.47      0.37      0.29       346\n",
      "weighted avg       0.83      0.61      0.62       346\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edson\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "for feature in [\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\", \"class\"]:\n",
    "    dataset[feature] = dataset[feature].astype('category')\n",
    "    dataset[feature] = dataset[feature].cat.codes\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "accuracy = [];\n",
    "\n",
    "for _ in range(0,5):\n",
    "    result = next(kf.split(df), None)\n",
    "    train = dataset.iloc[result[0]]\n",
    "    test =  dataset.iloc[result[1]]\n",
    "    \n",
    "    target_train = train['class']\n",
    "    target_test = test['class']\n",
    "    \n",
    "    train = train[[\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\"]]\n",
    "    test = test[[\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\"]]\n",
    "    \n",
    "    naive_bayes = GaussianNB()\n",
    "    naive_bayes.fit(train, target_train)\n",
    "    target_pred = naive_bayes.predict(test)\n",
    "    \n",
    "    accuracy.append(accuracy_score(target_test, target_pred))\n",
    "\n",
    "print(accuracy)\n",
    "# Fazer média da acuracia\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(target_pred, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty range for randrange()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-bc77354dac0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplitDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.67\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[0msummarize_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msummarizeByClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetPredictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummarize_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-39-bc77354dac0c>\u001b[0m in \u001b[0;36msplitDataset\u001b[1;34m(dataset, splitRatio)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcopy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainSet\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtrainSize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mtrainSet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtrainSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\random.py\u001b[0m in \u001b[0;36mrandrange\u001b[1;34m(self, start, stop, step, _int)\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mistart\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mistart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"empty range for randrange()\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;31m# stop argument supplied.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: empty range for randrange()"
     ]
    }
   ],
   "source": [
    "def splitDataset(dataset, splitRatio):\n",
    "    trainSize = int(len(dataset) * splitRatio)\n",
    "    trainSet = []\n",
    "    copy = list(dataset)\n",
    "    while len(trainSet) < trainSize:\n",
    "        index = random.randrange(len(copy))\n",
    "        trainSet.append(copy.pop(index))\n",
    "    return [trainSet, copy]\n",
    "\n",
    "def mean(numbers):\n",
    "    return sum(numbers)/float(len(numbers))\n",
    " \n",
    "def stdev(numbers):\n",
    "    avg = mean(numbers)\n",
    "    variance = sum([pow(x-avg,2) for x in numbers])/float(len(numbers)-1)\n",
    "    return math.sqrt(variance)\n",
    "\n",
    "def separateByClass(dataset):\n",
    "    separated = {}\n",
    "    for i in range(len(dataset)):\n",
    "        vector = dataset[i]\n",
    "        if (vector[-1] not in separated):\n",
    "            separated[vector[-1]] = []\n",
    "        separated[vector[-1]].append(vector)\n",
    "    return separated\n",
    "\n",
    "def summarize(dataset):\n",
    "    summaries = [(mean(attribute), stdev(attribute)) for attribute in zip(*dataset)]\n",
    "    del summaries[-1]\n",
    "    return summaries\n",
    "\n",
    "def summarizeByClass(dataset):\n",
    "    separated = separateByClass(dataset)\n",
    "    summaries = {}\n",
    "    for classValue, instances in separated.items():\n",
    "        summaries[classValue] = summarize(instances)\n",
    "    return summaries\n",
    "\n",
    "def calculateProbability(x, mean, stdev):\n",
    "    exponent = math.exp(-(math.pow(x-mean,2)/(2*math.pow(stdev,2))))\n",
    "    return (1 / (math.sqrt(2*math.pi) * math.pow(stdev, 2))) * exponent\n",
    "\n",
    "def calculateClassProbabilities(summaries, inputVector):\n",
    "    probabilities = {}\n",
    "    for classValue, classSummaries in summaries.items():\n",
    "        probabilities[classValue] = 1\n",
    "        for i in range(len(classSummaries)):\n",
    "            mean, stdev = classSummaries[i]\n",
    "            x = inputVector[i]\n",
    "            probabilities[classValue] *= calculateProbability(x, mean, stdev)\n",
    "    return probabilities\n",
    "\n",
    "def predict(summaries, inputVector):\n",
    "    probabilities = calculateClassProbabilities(summaries, inputVector)\n",
    "    bestLabel, bestProb = None, -1\n",
    "    for classValue, probability in probabilities.items():\n",
    "        if bestLabel is None or probability > bestProb:\n",
    "            bestProb = probability\n",
    "            bestLabel = classValue\n",
    "    return bestLabel\n",
    "\n",
    "def getPredictions(summaries, testSet):\n",
    "    predictions = []\n",
    "    for i in range(len(testSet)):\n",
    "        result = predict(summaries, testSet[i])\n",
    "        predictions.append(result)\n",
    "    return predictions\n",
    "\n",
    "def getAccuracy(testSet, predictions):\n",
    "    correct = 0\n",
    "    for i in range(len(testSet)):\n",
    "        if testSet[i][-1] == predictions[i]:\n",
    "            correct += 1\n",
    "    return (correct/float(len(testSet))) * 100.0\n",
    "\n",
    "train, test = splitDataset(dataset, 0.67)\n",
    "summarize_classes = summarizeByClass(dataset)\n",
    "predictions = getPredictions(summarize_classes, test)\n",
    "accuracy = getAccuracy(test, predictions)\n",
    "print(\"Accuracy: {0}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'buying': {3: 432, 0: 432, 2: 432, 1: 432},\n",
       " 'maint': {3: 432, 0: 432, 2: 432, 1: 432},\n",
       " 'doors': {0: 432, 1: 432, 2: 432, 3: 432},\n",
       " 'persons': {0: 576, 1: 576, 2: 576},\n",
       " 'lug_boot': {2: 576, 1: 576, 0: 576},\n",
       " 'safety': {1: 576, 2: 576, 0: 576},\n",
       " 'class': {2: 1210, 0: 384, 3: 65, 1: 69},\n",
       " 0: {'buying': {3: 0, 0: 0, 2: 0, 1: 0},\n",
       "  'maint': {2: 0, 1: 0, 0: 0, 3: 0},\n",
       "  'doors': {0: 0, 1: 0, 2: 0, 3: 0},\n",
       "  'persons': {1: 0, 2: 0},\n",
       "  'lug_boot': {2: 0, 1: 0, 0: 0},\n",
       "  'safety': {0: 0, 2: 0},\n",
       "  'class': {0: 0}},\n",
       " 1: {'buying': {2: 0, 1: 0},\n",
       "  'maint': {1: 0, 2: 0},\n",
       "  'doors': {0: 0, 1: 0, 2: 0, 3: 0},\n",
       "  'persons': {1: 0, 2: 0},\n",
       "  'lug_boot': {2: 0, 1: 0, 0: 0},\n",
       "  'safety': {0: 0, 2: 0},\n",
       "  'class': {1: 0}},\n",
       " 2: {'buying': {3: 0, 0: 0, 2: 0, 1: 0},\n",
       "  'maint': {3: 0, 0: 0, 2: 0, 1: 0},\n",
       "  'doors': {0: 0, 1: 0, 2: 0, 3: 0},\n",
       "  'persons': {0: 0, 1: 0, 2: 0},\n",
       "  'lug_boot': {2: 0, 1: 0, 0: 0},\n",
       "  'safety': {1: 0, 2: 0, 0: 0},\n",
       "  'class': {2: 0}},\n",
       " 3: {'buying': {2: 0, 1: 0},\n",
       "  'maint': {2: 0, 1: 0, 0: 0},\n",
       "  'doors': {0: 0, 1: 0, 2: 0, 3: 0},\n",
       "  'persons': {1: 0, 2: 0},\n",
       "  'lug_boot': {0: 0, 1: 0},\n",
       "  'safety': {0: 0},\n",
       "  'class': {3: 0}}}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Montando o sumário pelas classes\n",
    "summary_by_class = {0:[], 1:[], 2:[], 3:[]}\n",
    "for i in range(0, dataset.shape[0]):\n",
    "    array = [x for x in np.array(dataset.iloc[i])]\n",
    "    summary_by_class[array[-1]].append(array)\n",
    "\n",
    "# Tabela de probabilidades\n",
    "# entender melhor como vai ficar isso aqui\n",
    "# inicialização do dicionário\n",
    "model = {}\n",
    "# vê se não tem como reduzir a só esses dois laços\n",
    "# fazer isso pro sumario das classes\n",
    "for col in columns:\n",
    "    model[col] = {}\n",
    "    for i in range(0, dataset.shape[0]):\n",
    "        model[col][dataset.loc[i, col]] = 0\n",
    "        \n",
    "for col in columns:\n",
    "    for i in range(0, dataset.shape[0]):\n",
    "        model[col][dataset.loc[i, col]] += 1\n",
    "\n",
    "# contagem de cada valor por classe e coluna\n",
    "for classes in summary_by_class.keys():\n",
    "    model[classes] = {}\n",
    "    cont = 0\n",
    "    for col in columns:\n",
    "        model[classes][col] = {}\n",
    "        for i in range(0, len(summary_by_class[classes])):\n",
    "            model[classes][col][summary_by_class[classes][i][cont]] = 0\n",
    "            #print(summary_by_class[classes][i])\n",
    "            #print(summary_by_class[classes][i][cont])\n",
    "        cont += 1\n",
    "        #    model[i][col][dataset.loc[i, col]] = 1\n",
    "#summary_by_class[1][0]\n",
    "\n",
    "model\n",
    "\n",
    "\n",
    "dataset = [float(dataset[i][j]) for i in linhas for j in colunas]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
